{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#credentials do use Twitter API\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_token = \"\"\n",
    "access_token_secret = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing authentication into Twitter API\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "token = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will define search by key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = ('covid OR covid-19 OR corona OR coronavirus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will perform search by key word\n",
    "# result_type --> mixed (default), recent, popular\n",
    "# tweets --> this variable stores all tweets within the chosen key word\n",
    "# count --> total of tweets that will be collected, default is 100\n",
    "# Lang --> its is possible to specify a language to recover tweets\n",
    "\n",
    "tweets = token.search(q=keyword,count=10,result_type='mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "#just evaluating the total amount of tweets collected\n",
    "\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: {'HillaryClinton'} \n",
      "tweet: {'This presidency, summed up: \\n\\nWhile the country hit 154,000 COVID-19 deaths, the jobless rate soared, and an evictiâ€¦ https://t.co/3sBg4w10zz'}\n",
      "user: {'charliekirk11'} \n",
      "tweet: {'Why are Fauci, the head of the FDA, and Admiral Giroir slandering Hydroxychloroquine when a top epidemiologist fromâ€¦ https://t.co/dKnz2BTzPm'}\n",
      "user: {'RandPaul'} \n",
      "tweet: {'FL and NY have an identical number of per capita coronavirus infections but FL has 5X less per capita deaths. Willâ€¦ https://t.co/Rq5P1LZAVz'}\n",
      "user: {'NYSEDISBROKEN'} \n",
      "tweet: {'RT @nytopinion: The failure of the president and Republican lawmakers to provide relief to unemployed Americans is right up there with theâ€¦'}\n",
      "user: {'Pudingtane'} \n",
      "tweet: {\"RT @CarolM39: @G_TheOriginal @Angelicanang Yes, I would absolutely take HCQ if rx'd to me.  It has an extremely long safety record.  Also,â€¦\"}\n",
      "user: {'_thank_me_l8r'} \n",
      "tweet: {'Leave TikTok ALONE and fix the economic problems caused by COVID 19 https://t.co/Y2KlX4cpuM'}\n",
      "user: {'gbjp3321'} \n",
      "tweet: {'RT @drsimonegold: The Lancet study, which supposedly involved 96,000 patients was one of the first to openly discredit HCQ, claiming it wasâ€¦'}\n",
      "user: {'BarbUtesch'} \n",
      "tweet: {'RT @peterbakernyt: Trump: â€œCountries where there have been very significant flareups over the last short period of time are Spain, Germany,â€¦'}\n",
      "user: {'Robert_latino'} \n",
      "tweet: {'RT @Defensoria_Peru: #LaLibertad ðŸš¨Desde @Defensoria_Peru pedimos buscar soluciones para la falta de capacidad hospitalaria y de #oxÃ­geno meâ€¦'}\n",
      "user: {'BiekerAnnie'} \n",
      "tweet: {'RT @charliekirk11: Democrats put COVID-infected patients in nursing homes but arenâ€™t okay with sending healthy kids back to school?\\n\\nðŸ¤”'}\n"
     ]
    }
   ],
   "source": [
    "#Let's print, for example, more details about the user that tweeted and the content tweeted\n",
    "\n",
    "for tweet in tweets:\n",
    "    print(\"user: %s \"% {tweet.user.screen_name})\n",
    "    print(\"tweet: %s\" % {tweet.text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will initiate the polarity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "from textblob import TextBlob as tb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable to store polarities\n",
    "analysis = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an empty list to store scores\n",
    "tweets_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** This presidency, summed up: \n",
      "\n",
      "While the country hit 154,000 COVID-19 deaths, the jobless rate soared, and an evictiâ€¦ https://t.co/3sBg4w10zz\n",
      "** Why are Fauci, the head of the FDA, and Admiral Giroir slandering Hydroxychloroquine when a top epidemiologist fromâ€¦ https://t.co/dKnz2BTzPm\n",
      "** FL and NY have an identical number of per capita coronavirus infections but FL has 5X less per capita deaths. Willâ€¦ https://t.co/Rq5P1LZAVz\n",
      "** RT @nytopinion: The failure of the president and Republican lawmakers to provide relief to unemployed Americans is right up there with theâ€¦\n",
      "** RT @CarolM39: @G_TheOriginal @Angelicanang Yes, I would absolutely take HCQ if rx'd to me.  It has an extremely long safety record.  Also,â€¦\n",
      "** Leave TikTok ALONE and fix the economic problems caused by COVID 19 https://t.co/Y2KlX4cpuM\n",
      "** RT @drsimonegold: The Lancet study, which supposedly involved 96,000 patients was one of the first to openly discredit HCQ, claiming it wasâ€¦\n",
      "** RT @peterbakernyt: Trump: â€œCountries where there have been very significant flareups over the last short period of time are Spain, Germany,â€¦\n",
      "** RT @Defensoria_Peru: #LaLibertad ðŸš¨Desde @Defensoria_Peru pedimos buscar soluciones para la falta de capacidad hospitalaria y de #oxÃ­geno meâ€¦\n",
      "** RT @charliekirk11: Democrats put COVID-infected patients in nursing homes but arenâ€™t okay with sending healthy kids back to school?\n",
      "\n",
      "ðŸ¤”\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweets:\n",
    "    print('**',tweet.text)\n",
    "    analysis = tb(tweet.text)\n",
    "    polarity = analysis.sentiment.polarity\n",
    "    tweets_score.append(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity vectors:  [0.0, 0.5, -0.16666666666666666, -0.015476190476190504, 0.07500000000000001, 0.2, 0.125, 0.1625, 0.0, 0.3333333333333333]\n"
     ]
    }
   ],
   "source": [
    "# printing the polarity vectors\n",
    "print('Polarity vectors: ', tweets_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment AVG: 0.12136904761904761\n"
     ]
    }
   ],
   "source": [
    "# printing polarity average score for all collected tweets\n",
    "print('Sentiment AVG: ' + str(np.mean(tweets_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting if a tweet is not in english and translating it before and then applying sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated to english RT @Defensoria_Peru: #LaLiberty ðŸš¨From @Defensoria_Peru we ask to find solutions for the lack of hospital capacity and # oxygen ...:\n"
     ]
    }
   ],
   "source": [
    "polarities =[]\n",
    "\n",
    "for tweet in tweets:\n",
    "    analysis = tb(tweet.text)\n",
    "    if analysis.detect_language()!='en':\n",
    "        translate = tb(str(analysis.translate(to='en')))\n",
    "        \n",
    "        print ('Translated to english %s:' % translate)\n",
    "        \n",
    "        polarity = translate.sentiment.polarity\n",
    "    else:\n",
    "        polarity = analysis.sentiment.polarity\n",
    "    polarities.append(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarities vector before translating: [0.0, 0.5, -0.16666666666666666, -0.015476190476190504, 0.07500000000000001, 0.2, 0.125, 0.1625, 0.0, 0.3333333333333333]\n",
      "Polarities vector after translating: [0.0, 0.5, -0.16666666666666666, -0.015476190476190504, 0.07500000000000001, 0.2, 0.125, 0.1625, 0.0, 0.3333333333333333]\n"
     ]
    }
   ],
   "source": [
    "print('Polarities vector before translating:',tweets_score)\n",
    "print('Polarities vector after translating:', polarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment AVG after translating: 0.12136904761904761\n"
     ]
    }
   ],
   "source": [
    "# printing polarity average score for all collected tweets after translating\n",
    "print('Sentiment AVG after translating: ' + str(np.mean(polarities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store collected tweets into JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing json package\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = tweets[0]\n",
    "\n",
    "#convert to string\n",
    "json_str = json.dumps(status._json)\n",
    "\n",
    "parsed = json.loads(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweepy.models.SearchResults"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetPath = r'C:\\Users\\Ivan Mello\\Documents\\90 - Bootcamp IGTI CiÃªncia de Dados\\MÃ³dulo 2\\JSON_tweets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datasetPath+'\\\\tweets_keywords.json','a',encoding='utf8') as filename:\n",
    "    for tweet in tweets:\n",
    "        status = tweet\n",
    "        \n",
    "        #convert to string\n",
    "        json_str = json.dumps(status._json)\n",
    "        \n",
    "        #desrialize the string to convert it to a dict\n",
    "        parsed = json.loads(json_str)\n",
    "        \n",
    "        #store tweet into the json file\n",
    "        json.dump(parsed, filename, ensure_ascii=False, sort_keys=True, indent=4, separators=(',',':'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These next steps are being performed to store JSON into a MongoDB database called twitterdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the MongoDB local database\n",
    "con = pymongo.MongoClient('localhost', 27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting the database we need to input data\n",
    "db = con.twitterdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selection the collection\n",
    "collection = db.tweets_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ivan Mello\\Anaconda3\\envs\\dev\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: count is deprecated. Use estimated_document_count or count_documents instead. Please note that $where must be replaced by $expr, $near must be replaced by $geoWithin with $center, and $nearSphere must be replaced by $geoWithin with $centerSphere\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting how many items are inside the collection, just to show the error.\n",
    "# we need to use count_documents\n",
    "db.tweets_keywords.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting how many items are inside the collection, just to show how to not show the error\n",
    "db.tweets_keywords.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserting tweets data into the tweets_keywords collections inside MongoDB localserver\n",
    "i=0\n",
    "for tweet in tweets:\n",
    "    db.tweets_keywords.insert_one(tweet._json)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
